{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ba0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dev/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import reuters, movie_reviews\n",
    "from collections import Counter, defaultdict\n",
    "import math, random, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# nltk.download('reuters')\n",
    "# nltk.download('movie_reviews')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab') \n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11032b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(reuters.words(categories='acq')[:5000])\n",
    "docs = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "tokens = ['<s>'] + [w.lower() for sent in docs for w in sent] + ['</s>']\n",
    "\n",
    "def build_ngram_counts(tokens, n):\n",
    "    return Counter(ngrams(tokens, n))\n",
    "\n",
    "unigram_counts = build_ngram_counts(tokens, 1)\n",
    "bigram_counts = build_ngram_counts(tokens, 2)\n",
    "trigram_counts = build_ngram_counts(tokens, 3)\n",
    "\n",
    "V = len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "220e62d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob(w1, w2):\n",
    "    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[(w1,)] + V)\n",
    "\n",
    "def sentence_prob(sentence):\n",
    "    sent = ['<s>'] + nltk.word_tokenize(sentence.lower()) + ['</s>']\n",
    "    prob = 1\n",
    "    for i in range(1, len(sent)):\n",
    "        prob *= bigram_prob(sent[i-1], sent[i])\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffe060d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(the company made a profit) = 4.456698783065934e-18\n",
      "P(profit company the made) = 2.093162861779967e-16\n"
     ]
    }
   ],
   "source": [
    "s1 = 'the company made a profit'\n",
    "s2 = 'profit company the made'\n",
    "print(f'P({s1}) =', sentence_prob(s1))\n",
    "print(f'P({s2}) =', sentence_prob(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6de0a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(test_sents):\n",
    "    # test_sents: list of tokenized sentences (tokens already)\n",
    "    N = sum(len(s) for s in test_sents)\n",
    "    log_prob = 0.0\n",
    "    for s in test_sents:\n",
    "        sent = ['<s>'] + s + ['</s>']\n",
    "        for i in range(1, len(sent)):\n",
    "            log_prob += math.log(bigram_prob(sent[i-1], sent[i]))\n",
    "    return math.exp(-log_prob / N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd99e1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3819.603289525883\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\n",
    "    [\"the\", \"company\", \"announced\", \"a\", \"dividend\"],\n",
    "    [\"the\", \"stock\", \"rose\", \"after\", \"the\", \"report\"],\n",
    "    [\"the\", \"market\", \"closed\", \"higher\", \"today\"],\n",
    "    [\"analysts\", \"expect\", \"further\", \"gains\"],\n",
    "    [\"the\", \"bank\", \"reported\", \"quarterly\", \"profits\"],\n",
    "    [\"investors\", \"were\", \"encouraged\", \"by\", \"the\", \"results\"],\n",
    "    [\"shares\", \"of\", \"the\", \"company\", \"increased\"],\n",
    "    [\"economic\", \"growth\", \"remains\", \"strong\"],\n",
    "    [\"pacificorp\", \"said\", \"it\", \"plans\", \"new\", \"investments\"],\n",
    "    [\"the\", \"deal\", \"was\", \"approved\", \"by\", \"regulators\"]\n",
    "]\n",
    "\n",
    "print(\"Perplexity:\", perplexity(test_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96414165",
   "metadata": {},
   "source": [
    "### Bigram Language Model with Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38ae31aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens after stopword removal: 2621\n",
      "P(the company made a profit) = 5.131200021554437e-16\n",
      "P(profit company the made) = 5.844418804532868e-13\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load and tokenize text\n",
    "text = \" \".join(reuters.words(categories='acq')[:5000])\n",
    "docs = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "# Lowercase and remove stopwords\n",
    "tokens = ['<s>'] + [w.lower() for sent in docs for w in sent if w.lower() not in stop_words and w.isalpha()] + ['</s>']\n",
    "\n",
    "print(\"Total tokens after stopword removal:\", len(tokens))\n",
    "\n",
    "unigram_counts = build_ngram_counts(tokens, 1)\n",
    "bigram_counts = build_ngram_counts(tokens, 2)\n",
    "trigram_counts = build_ngram_counts(tokens, 3)\n",
    "\n",
    "V = len(set(tokens))\n",
    "\n",
    "# Sentence probability using bigrams\n",
    "def sentence_prob(sentence):\n",
    "    sent = ['<s>'] + nltk.word_tokenize(sentence.lower()) + ['</s>']\n",
    "    prob = 1\n",
    "    for i in range(1, len(sent)):\n",
    "        if sent[i].isalpha():  # ignore punctuation\n",
    "            prob *= bigram_prob(sent[i-1], sent[i])\n",
    "    return prob\n",
    "\n",
    "# Example sentences\n",
    "s1 = 'the company made a profit'\n",
    "s2 = 'profit company the made'\n",
    "\n",
    "print(f'P({s1}) =', sentence_prob(s1))\n",
    "print(f'P({s2}) =', sentence_prob(s2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275ad4b",
   "metadata": {},
   "source": [
    "### Naive Bayes Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b872d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [(list(movie_reviews.words(fileid)), category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "train, test = train_test_split(docs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "82f8cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36352\n",
      "{'pos': 668319, 'neg': 598082}\n"
     ]
    }
   ],
   "source": [
    "word_counts = {'pos': Counter(), 'neg': Counter()}\n",
    "for words, label in train:\n",
    "    word_counts[label].update(w.lower() for w in words)\n",
    "\n",
    "V = len(set(word_counts['pos']) | set(word_counts['neg']))\n",
    "total_counts = {c: sum(word_counts[c].values()) for c in ['pos','neg']}\n",
    "\n",
    "print(f\"Vocabulary size: {V}\")\n",
    "print(total_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7748b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_word_given_class(word, c):\n",
    "    return (word_counts[c][word] + 1) / (total_counts[c] + V)\n",
    "\n",
    "def P_class_given_doc(words):\n",
    "    probs = {}\n",
    "    for c in ['pos', 'neg']:\n",
    "        log_prob = math.log(0.5)\n",
    "        for w in words:\n",
    "            log_prob += math.log(P_word_given_class(w, c))\n",
    "        probs[c] = log_prob\n",
    "    return max(probs, key=probs.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "47a26466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.805\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for words, label in test[:200]:\n",
    "    pred = P_class_given_doc(words)\n",
    "    correct += (pred == label)\n",
    "print('Accuracy:', correct / len(test[:200]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82967ac",
   "metadata": {},
   "source": [
    "### Laxicons and negation handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "054733da",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_lexicon = {'good', 'excellent', 'great', 'amazing', 'love', 'nice'}\n",
    "negative_lexicon = {'bad', 'awful', 'terrible', 'hate', 'boring', 'poor'}\n",
    "\n",
    "def add_lexicon_tokens(words):\n",
    "    if any(w in positive_lexicon for w in words):\n",
    "        words.append('LEX_POS')\n",
    "    if any(w in negative_lexicon for w in words):\n",
    "        words.append('LEX_NEG')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1e8d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['I', 'did', 'not', 'like', 'this', 'movie,', 'it', 'was', 'not', 'good', 'at', 'all.']\n",
      "Augmented tokens: ['i', 'did', 'not', 'NOT_like', 'NOT_this', 'NOT_movie', 'it', 'was', 'not', 'NOT_good', 'NOT_at', 'NOT_all']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "negation_words = {'not', \"n't\", 'never', 'no'}\n",
    "positive_lexicon = {'good', 'excellent', 'great', 'amazing', 'love', 'nice'}\n",
    "negative_lexicon = {'bad', 'awful', 'terrible', 'hate', 'boring', 'poor'}\n",
    "\n",
    "def apply_negation_and_lexicon(words):\n",
    "    out = []\n",
    "    negate = False\n",
    "    for w in words:\n",
    "        lw = w.lower().strip(string.punctuation)\n",
    "\n",
    "        if lw in negation_words:\n",
    "            negate = True\n",
    "            out.append(lw)\n",
    "            continue\n",
    "\n",
    "        # If word ends with punctuation â†’ stop negation\n",
    "        if any(w.endswith(p) for p in ['.', '!', '?', ',', ';', ':']):\n",
    "            end_punct = True\n",
    "        else:\n",
    "            end_punct = False\n",
    "\n",
    "        if negate and lw.isalpha():\n",
    "            out.append('NOT_' + lw)\n",
    "        else:\n",
    "            out.append(lw)\n",
    "\n",
    "        # Stop negation after punctuation mark\n",
    "        if end_punct:\n",
    "            negate = False\n",
    "\n",
    "    # Add lexicon tokens\n",
    "    # clean_words = [x.lower().lstrip('not_') for x in out]\n",
    "    clean_words = out\n",
    "    if any(w in positive_lexicon for w in clean_words):\n",
    "        out.append('LEX_POS')\n",
    "    if any(w in negative_lexicon for w in clean_words):\n",
    "        out.append('LEX_NEG')\n",
    "\n",
    "    return out\n",
    "\n",
    "# Try on a sample sentence\n",
    "sample = \"I did not like this movie, it was not good at all.\"\n",
    "print('Original tokens:', sample.split())\n",
    "print('Augmented tokens:', apply_negation_and_lexicon(sample.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8c0f870f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with negation+lexicon (first 200 test docs): 0.805\n"
     ]
    }
   ],
   "source": [
    "# Evaluate NB with negation + lexicon on a small subset\n",
    "# Build augmented training counts\n",
    "word_counts_aug = {'pos': Counter(), 'neg': Counter()}\n",
    "for words, label in train:\n",
    "    aug = apply_negation_and_lexicon(words)\n",
    "    word_counts_aug[label].update(aug)\n",
    "\n",
    "V_aug = len(set(word_counts_aug['pos']) | set(word_counts_aug['neg']))\n",
    "\n",
    "total_aug = {c: sum(word_counts_aug[c].values()) for c in ['pos','neg']}\n",
    "\n",
    "def P_word_given_class_aug(word, c):\n",
    "    return (word_counts_aug[c][word] + 1) / (total_aug[c] + V_aug)\n",
    "\n",
    "def P_class_given_doc_aug(words):\n",
    "    log_probs = {}\n",
    "    for c in ['pos','neg']:\n",
    "        log_prob = math.log(0.5)\n",
    "        for w in apply_negation_and_lexicon(words):\n",
    "            log_prob += math.log(P_word_given_class_aug(w, c))\n",
    "        log_probs[c] = log_prob\n",
    "    return max(log_probs, key=log_probs.get)\n",
    "\n",
    "# Test on subset\n",
    "correct = 0\n",
    "for words, label in test[:200]:\n",
    "    pred = P_class_given_doc_aug(words)\n",
    "    correct += (pred == label)\n",
    "print('Accuracy with negation+lexicon (first 200 test docs):', correct / 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c16dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
