{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60006873",
   "metadata": {},
   "source": [
    "# ðŸ§ª NLP Lab: N-Gram Language Modeling and NaÃ¯ve Bayes Text Classification\n",
    "\n",
    "This notebook is a step-by-step lab for students. It contains *theory*, *formulas*, explanations, and runnable *code cells* for each part.\n",
    "\n",
    "Sections:\n",
    "\n",
    "1. Setup\n",
    "2. N-gram language modeling (unigram/bigram/trigram)\n",
    "3. Add-1 (Laplace) smoothing and sentence probability\n",
    "4. Perplexity calculation\n",
    "5. NaÃ¯ve Bayes text classification (with negation & lexicon features)\n",
    "6. Exercises and deliverables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c209e5",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "- Build unigram, bigram, and trigram language models from text.\n",
    "- Implement additive (Laplace) smoothing and compute sentence probabilities.\n",
    "- Compute perplexity as an intrinsic evaluation metric.\n",
    "- Use NaÃ¯ve Bayes for sentiment classification on a small corpus.\n",
    "- Explore negation handling and lexicon feature augmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb6057",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ Setup\n",
    "\n",
    "Run the cell below to install required packages and import libraries. If you're using Google Colab the `!pip install` will be fast. On local machines, ensure you have network access to download NLTK corpora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q nltk pandas numpy scikit-learn matplotlib nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7305eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import reuters, movie_reviews\n",
    "from collections import Counter, defaultdict\n",
    "import math, random, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download corpora (only the first time)\n",
    "# nltk.download('reuters')\n",
    "# nltk.download('movie_reviews')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7189231",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1 â€” Tokenization & N-gram Counts\n",
    "\n",
    "**Goal:** create unigram, bigram, and trigram counts from a corpus (Reuters `acq` category).\n",
    "\n",
    "**Notation / formulas**:\n",
    "\n",
    "- Let \\(w_i\\) be the word at position \\(i\\).\n",
    "- Unigram probability (maximum likelihood estimate):\n",
    "\n",
    "\\[ P(w) = \\frac{C(w)}{N} \\]\n",
    "\n",
    "where \\(C(w)\\) is the count of \\(w\\) and \\(N\\) is total tokens.\n",
    "\n",
    "- Bigram conditional probability (MLE):\n",
    "\n",
    "\\[ P(w_i \\mid w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})} \\]\n",
    "\n",
    "We'll compute counts with NLTK and `collections.Counter`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4994b5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size V = 1319\n",
      "Sample unigram counts (top 10): [(('.',), 264), (('the',), 223), ((',',), 183), (('of',), 124), (('to',), 95), (('said',), 93), (('a',), 90), (('and',), 75), (('in',), 74), (('s',), 67)]\n",
      "Sample bigram counts (top 10): [((\"'\", 's'), 50), (('&', 'lt'), 45), (('lt', ';'), 45), (('of', 'the'), 24), (('.', 'the'), 23), (('said', '.'), 19), (('said', 'it'), 19), (('.', '``'), 16), (('.', 's'), 16), ((',', 'the'), 16)]\n"
     ]
    }
   ],
   "source": [
    "# Load sentences from Reuters (acq category)\n",
    "text = \" \".join(reuters.words(categories='acq')[:5000])\n",
    "docs = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "# Flatten tokens and add sentence boundary tokens\n",
    "tokens = ['<s>'] + [w.lower() for sent in docs for w in sent] + ['</s>']\n",
    "\n",
    "# Helper to build n-gram counts\n",
    "def build_ngram_counts(tokens, n):\n",
    "    return Counter(ngrams(tokens, n))\n",
    "\n",
    "unigram_counts = build_ngram_counts(tokens, 1)\n",
    "bigram_counts = build_ngram_counts(tokens, 2)\n",
    "trigram_counts = build_ngram_counts(tokens, 3)\n",
    "\n",
    "V = len(set(tokens))  # vocabulary size\n",
    "\n",
    "print('Vocabulary size V =', V)\n",
    "print('Sample unigram counts (top 10):', unigram_counts.most_common(10))\n",
    "print('Sample bigram counts (top 10):', bigram_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429967d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 â€” Laplace (Add-1) Smoothing\n",
    "\n",
    "**Problem:** if a bigram \\((w_{i-1}, w_i)\\) never appears in training, \\(C(w_{i-1}, w_i)=0\\) and the MLE gives zero probability.\n",
    "\n",
    "**Laplace (Add-1) smoothing** adds 1 to every bigram count to avoid zeros:\n",
    "\n",
    "\\[ P_{Laplace}(w_i \\mid w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V} \\]\n",
    "\n",
    "where \\(V\\) is the vocabulary size (number of distinct tokens).\n",
    "\n",
    "This guarantees every possible next word has non-zero probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f25d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(company|the) = 0.009727626459143969\n",
      "P(made|company) = 0.0007446016381236039\n",
      "P(a|made) = 0.0015117157974300832\n",
      "P(profit|a) = 0.0007097232079489\n",
      "P(</s>|profit) = 0.000757002271006813\n"
     ]
    }
   ],
   "source": [
    "# Define smoothed bigram probability\n",
    "\n",
    "def bigram_prob(w1, w2, bigram_counts=bigram_counts, unigram_counts=unigram_counts, V=V):\n",
    "    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[(w1,)] + V)\n",
    "\n",
    "# Example: show probabilities for some bigrams\n",
    "examples = [('the', 'company'), ('company', 'made'), ('made', 'a'), ('a', 'profit'), ('profit','</s>')]\n",
    "for a, b in examples:\n",
    "    print(f\"P({b}|{a}) =\", bigram_prob(a, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625758d4",
   "metadata": {},
   "source": [
    "### Sentence probability (chain rule using bigrams)\n",
    "\n",
    "Given a sentence \\(w_1, w_2, \\dots, w_n\\) with start token `<s>` and end token `</s>`,\n",
    "\n",
    "\\[ P(s) = \\prod_{i=1}^{n} P(w_i \\mid w_{i-1}) \\]\n",
    "\n",
    "We compute this using the smoothed bigram probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9122de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: the company made a profit\n",
      "  Prob = 4.457e-18, log-prob = -39.952\n",
      "Sentence: profit company the made\n",
      "  Prob = 2.093e-16, log-prob = -36.103\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def sentence_prob(sentence, tokenizer=nltk.word_tokenize):\n",
    "    sent = ['<s>'] + tokenizer(sentence.lower()) + ['</s>']\n",
    "    log_prob = 0.0\n",
    "    for i in range(1, len(sent)):\n",
    "        p = bigram_prob(sent[i-1], sent[i])\n",
    "        log_prob += math.log(p)\n",
    "    return math.exp(log_prob), log_prob  # return probability and log-prob\n",
    "\n",
    "s1 = 'the company made a profit'\n",
    "s2 = 'profit company the made'\n",
    "\n",
    "p1, lp1 = sentence_prob(s1)\n",
    "p2, lp2 = sentence_prob(s2)\n",
    "\n",
    "print(f\"Sentence: {s1}\\n  Prob = {p1:.3e}, log-prob = {lp1:.3f}\")\n",
    "print(f\"Sentence: {s2}\\n  Prob = {p2:.3e}, log-prob = {lp2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88796561",
   "metadata": {},
   "source": [
    "**Interpretation:** the grammatical sentence should have a higher probability (less negative log-prob). We often work with log-probabilities to avoid numeric underflow.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3 â€” Perplexity\n",
    "\n",
    "Perplexity measures how well a probability model predicts unseen text. For a test set of tokens of length \\(N\\) with log-probability \\(\\log P(W)\\):\n",
    "\n",
    "\\[ perplexity = \\exp\\left(-\\frac{1}{N} \\log P(W) \\right) \\]\n",
    "\n",
    "Lower perplexity indicates a better model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6108c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on 50 held-out Reuters sentences: 866.6795836049256\n"
     ]
    }
   ],
   "source": [
    "def perplexity(test_sents):\n",
    "    # test_sents: list of tokenized sentences (tokens already)\n",
    "    N = sum(len(s) + 1 for s in test_sents)  # +1 for </s>\n",
    "    log_prob = 0.0\n",
    "    for s in test_sents:\n",
    "        sent = ['<s>'] + s + ['</s>']\n",
    "        for i in range(1, len(sent)):\n",
    "            log_prob += math.log(bigram_prob(sent[i-1], sent[i]))\n",
    "    return math.exp(-log_prob / N)\n",
    "\n",
    "# Build some held-out test sentences (use last 50 sentences from Reuters)\n",
    "test_docs = [ [w.lower() for w in sent] for sent in reuters.sents(categories='acq')[2000:2050] ]\n",
    "print('Perplexity on 50 held-out Reuters sentences:', perplexity(test_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb290a7",
   "metadata": {},
   "source": [
    "### Exercise: Why smoothing improves perplexity?\n",
    "\n",
    "Because smoothing assigns non-zero probability to previously unseen n-grams, it avoids the model's total probability going to zero on test data, which would lead to infinite perplexity. Proper smoothing redistributes probability mass and typically lowers perplexity on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4 â€” NaÃ¯ve Bayes Text Classification\n",
    "\n",
    "**Goal:** classify movie reviews (positive/negative) using bag-of-words NaÃ¯ve Bayes with Laplace smoothing.\n",
    "\n",
    "**Model:**\n",
    "\n",
    "We want \\(P(c \\mid d) \\propto P(c) \\prod_{i} P(w_i \\mid c)\\),\n",
    "\n",
    "where we estimate:\n",
    "\n",
    "\\[ P(w \\mid c) = \\frac{C(w,c) + 1}{\\sum_{w'} C(w',c) + V} \\]\n",
    "\n",
    "and use log probabilities for numerical stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ded2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare movie reviews dataset\n",
    "docs = [(list(movie_reviews.words(fileid)), category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "train, test = train_test_split(docs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build class-specific word counts\n",
    "word_counts = {'pos': Counter(), 'neg': Counter()}\n",
    "for words, label in train:\n",
    "    word_counts[label].update(w.lower() for w in words)\n",
    "\n",
    "V_nb = len(set(word_counts['pos']) | set(word_counts['neg']))\n",
    "total_counts = {c: sum(word_counts[c].values()) for c in ['pos','neg']}\n",
    "\n",
    "print('V (vocab for NB) =', V_nb)\n",
    "print('Total token counts per class:', total_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9858ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_word_given_class(word, c):\n",
    "    return (word_counts[c][word] + 1) / (total_counts[c] + V_nb)\n",
    "\n",
    "def P_class_given_doc(words):\n",
    "    # returns the predicted class using log-probs\n",
    "    log_probs = {}\n",
    "    for c in ['pos', 'neg']:\n",
    "        log_prob = math.log(0.5)  # assume equal priors\n",
    "        for w in words:\n",
    "            log_prob += math.log(P_word_given_class(w.lower(), c))\n",
    "        log_probs[c] = log_prob\n",
    "    return max(log_probs, key=log_probs.get)\n",
    "\n",
    "# Evaluate on a subset of the test set\n",
    "correct = 0\n",
    "for words, label in test[:200]:\n",
    "    pred = P_class_given_doc(words)\n",
    "    correct += (pred == label)\n",
    "\n",
    "print('Accuracy (first 200 test docs):', correct / 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448bb8d4",
   "metadata": {},
   "source": [
    "### Negation Handling & Lexicon Features\n",
    "\n",
    "NaÃ¯ve Bayes can misclassify phrases like \"not good\" because it treats words independently. A common trick is to prepend a `NOT_` marker to words after a negation word until the next punctuation. Example:\n",
    "\n",
    "`I do not like this movie` â†’ tokens: `I do not NOT_like NOT_this NOT_movie`\n",
    "\n",
    "Lexicon features add a token if any known positive/negative words appear (`LEX_POS`, `LEX_NEG`) which gives the classifier a prior sentiment signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba1e19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['I', 'did', 'not', 'like', 'this', 'movie,', 'it', 'was', 'not', 'good', 'at', 'all.']\n",
      "Augmented tokens: ['i', 'did', 'not', 'NOT_like', 'NOT_this', 'movie,', 'NOT_it', 'NOT_was', 'not', 'NOT_good', 'NOT_at', 'all.', 'LEX_POS']\n"
     ]
    }
   ],
   "source": [
    "# Simple negation handling and lexicon augmentation\n",
    "negation_words = {'not', \"n't\", 'never', 'no'}\n",
    "positive_lexicon = {'good', 'excellent', 'great', 'amazing', 'love', 'nice'}\n",
    "negative_lexicon = {'bad', 'awful', 'terrible', 'hate', 'boring', 'poor'}\n",
    "\n",
    "import string\n",
    "\n",
    "def apply_negation_and_lexicon(words):\n",
    "    out = []\n",
    "    negate = False\n",
    "    for w in words:\n",
    "        lw = w.lower()\n",
    "        # break negation on sentence punctuation\n",
    "        if any(p in lw for p in ['.', '!', '?']):\n",
    "            negate = False\n",
    "        if lw in negation_words:\n",
    "            negate = True\n",
    "            out.append(lw)\n",
    "            continue\n",
    "        if negate and lw.isalpha():\n",
    "            out.append('NOT_' + lw)\n",
    "        else:\n",
    "            out.append(lw)\n",
    "    # lexicon tokens\n",
    "    if any(w in positive_lexicon for w in [x.lower().lstrip('not_') for x in out]):\n",
    "        out.append('LEX_POS')\n",
    "    if any(w in negative_lexicon for w in [x.lower().lstrip('not_') for x in out]):\n",
    "        out.append('LEX_NEG')\n",
    "    return out\n",
    "\n",
    "# Try on a sample sentence\n",
    "sample = \"I did not like this movie, it was not good at all.\"\n",
    "print('Original tokens:', sample.split())\n",
    "print('Augmented tokens:', apply_negation_and_lexicon(sample.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e836980",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Evaluate NB with negation + lexicon on a small subset\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Build augmented training counts\u001b[39;00m\n\u001b[32m      3\u001b[39m word_counts_aug = {\u001b[33m'\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m'\u001b[39m: Counter(), \u001b[33m'\u001b[39m\u001b[33mneg\u001b[39m\u001b[33m'\u001b[39m: Counter()}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m words, label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain\u001b[49m:\n\u001b[32m      5\u001b[39m     aug = apply_negation_and_lexicon(words)\n\u001b[32m      6\u001b[39m     word_counts_aug[label].update(aug)\n",
      "\u001b[31mNameError\u001b[39m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate NB with negation + lexicon on a small subset\n",
    "# Build augmented training counts\n",
    "word_counts_aug = {'pos': Counter(), 'neg': Counter()}\n",
    "for words, label in train:\n",
    "    aug = apply_negation_and_lexicon(words)\n",
    "    word_counts_aug[label].update(aug)\n",
    "\n",
    "V_aug = len(set(word_counts_aug['pos']) | set(word_counts_aug['neg']))\n",
    "\n",
    "total_aug = {c: sum(word_counts_aug[c].values()) for c in ['pos','neg']}\n",
    "\n",
    "def P_word_given_class_aug(word, c):\n",
    "    return (word_counts_aug[c][word] + 1) / (total_aug[c] + V_aug)\n",
    "\n",
    "def P_class_given_doc_aug(words):\n",
    "    log_probs = {}\n",
    "    for c in ['pos','neg']:\n",
    "        log_prob = math.log(0.5)\n",
    "        for w in apply_negation_and_lexicon(words):\n",
    "            log_prob += math.log(P_word_given_class_aug(w, c))\n",
    "        log_probs[c] = log_prob\n",
    "    return max(log_probs, key=log_probs.get)\n",
    "\n",
    "# Test on subset\n",
    "correct = 0\n",
    "for words, label in test[:200]:\n",
    "    pred = P_class_given_doc_aug(words)\n",
    "    correct += (pred == label)\n",
    "print('Accuracy with negation+lexicon (first 200 test docs):', correct / 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5f668",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Run Exercise 1 (sentence probs). Report which sentence has higher probability and why.\n",
    "2. Compute perplexity on a held-out set and compare vanilla bigram vs add-1 smoothed bigram (you can simulate zero-probability by removing some bigrams from the counts).\n",
    "3. Train NaÃ¯ve Bayes with and without stopword removal; compare accuracies.\n",
    "4. Try different lexicon sizes (small vs large) and observe impact.\n",
    "5. Implement interpolation between unigrams, bigrams, and trigrams using held-out data to tune weights.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- A filled notebook with outputs and short comments on observations.\n",
    "- A 1-page reflection comparing statistical (n-gram / NB) vs lexical approaches.\n",
    "\n",
    "---\n",
    "\n",
    "*End of lab.*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
